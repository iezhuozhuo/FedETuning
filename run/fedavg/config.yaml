data_config:
  dataset_name: glue


federated_config:
  clients_num: 10
  rounds: 10
  alpha: 1.0
  sample: 1.0


model_config:
  model_type: roberta
  model_output_mode: seq_classification
  permutation_layers: false
  client_model_layers: [0,1,2,3,4,5]
  server_model_layers: [0,1,2,3,4,5]
  tuning_type:
  unfrozen_modules: [layer_norm, final_layer_norm, classifier]
  bottleneck_dim: 24

training_config:
  per_device_train_batch_size: 32
  num_train_epochs: 1
  learning_rate: 5e-5
  metric_name: glue
  seed: 42

